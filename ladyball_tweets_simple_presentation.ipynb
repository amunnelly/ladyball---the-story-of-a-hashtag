{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ladyball - the Story of a Hashtag\n",
    "\n",
    "Anthony Munnelly\n",
    "\n",
    "[@anspailpin](https://twitter.com/anspailpin)\n",
    "\n",
    "https://ie.linkedin.com/in/anthonymunnelly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ladyball - Where It Begins\n",
    "On January 13th of this year of grace, 2016, at about lunchtime, the surfing classes of Erin got a shock. This shock:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Alanna, Ger, and Lynn](alanna_ger_lynn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nobody knew what to make of it, except that nobody seemed much to like it. Nobody knew what this Ladyball was for. People were almost certain that it was a joke, but for a joke it didn't seem all that funny.\n",
    "\n",
    "About a week later it transpired that Ladyball was a guerilla marketing campaign to announce the Lidl supermarket chain's sponsorship of Ladies' Gaelic Football. But the questions I want to figure out tonight are:\n",
    "1. Was the campaign a success?\n",
    "2. Can a search of the `#ladyball` hashtag on Twitter answer this question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tweet Metadata\n",
    "The [Twitter api](https://dev.twitter.com/rest/reference/get/statuses/user_timeline) shows a sample tweet, broken down as a json object. Tweets are famous for being short and (sometimes) sweet. But the amount of information that's packed into one tweet is staggering, and a little bit frightening.\n",
    "\n",
    "You can see a sample if you click the link above to the Twitter Developers but it's a long list of javascript objects, many of which contain javascript objects themselves, just like those Russian dolls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tweepy\n",
    "*Tweepy* is now the standard module for interacting with Twitter in Python. Each Tweepy search returns an iterable Tweepy result set object. Each iteration is a Tweepy object, that compares to the `.json` we saw just now like so:\n",
    "\n",
    "| .json | Tweepy |\n",
    "|-----|------|\n",
    "|`result[0]['created_at']`| result[0].created_at|\n",
    "|`result[0]['user']['screen_name']`| result[0].user.screen_name|\n",
    "\n",
    "### Collecting the Tweets\n",
    "In the current case, tweets were collected by running a program that had two functions. The first function created the api that allowed us to interact with Twitter in the first place, and the second then collected and pickled the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating the api\n",
    "You have to register an app with Twitter in order to use the api. Once created, your app supplies you with four passwords - consumer_key, consumer_secret, access_token, and access_token_secret. It is, of course, extremely bad practice to write these into any program, so I prefer to store them as .json and load them at the start. This is the straight-forward code to do so. In previous iterations of tweepy, it was necessary to set a rate-limit-checker yourself - this is now part of the package. Progress is a wonderful thing.\n",
    "```\n",
    "def create_the_api(my_login):\n",
    "\n",
    "    # Log in to Twitter\n",
    "    with open(my_login, 'r') as f:\n",
    "        keys = json.load(f)\n",
    "        \n",
    "    auth = tweepy.OAuthHandler(keys['consumer_key'],\n",
    "                               keys['consumer_secret'])\n",
    "    auth.set_access_token(keys['access_token'],\n",
    "                          keys['access_token_secret'])\n",
    "\n",
    "    return tweepy.API(auth, wait_on_rate_limit=True,\n",
    "                      wait_on_rate_limit_notify=True)\n",
    "\n",
    "api = create_the_api(MY_URL)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Twitter Rate Limits\n",
    "\n",
    "[Twitter limits api access](https://dev.twitter.com/rest/public/rate-limiting). Depending on what you're looking for, you can get fifteen or 180 requests every fifteen minutes. No more. So once we hit that limit, we have no option but to wait out the fifteen minutes until we're ready to go again, and we have to allow for that in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the tweet-finder function\n",
    "Having created the api function, this is the function that finds the tweets, with a helper function that I'm about to explain.\n",
    "```\n",
    "def max_id_finder(temp):\n",
    "    # A helper function to stop us collecting\n",
    "    # the same data over and over again\n",
    "    #(tweepy.Result) -> (int)\n",
    "    \n",
    "    ids = [int(tweet.id) for tweet in temp]\n",
    "    ids.sort()\n",
    "\n",
    "    return ids[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```\n",
    "def hashtag_searcher(hashtag, count=180, api=api):\n",
    "    if hashtag[0] == '#': # Allow for whether or not the\n",
    "                          # user has included # as part of\n",
    "                          # her hashtag\n",
    "        hashtag = '%23' + hashtag[1:]\n",
    "    else:\n",
    "        hashtag = '%23' + hashtag\n",
    "        \n",
    "    tweets = []\n",
    "    temp = []\n",
    "    for i in range(count):\n",
    "        print i\n",
    "        if temp != []: # Checks to see if loop has run already,\n",
    "                       # and thus needs the max_id parameter\n",
    "            temp = api.search(hashtag,\n",
    "                              max_id = max_id_finder(temp))\n",
    "        else:\n",
    "            temp = api.search(hashtag)\n",
    "        \n",
    "        [tweets.append(t) for t in temp]\n",
    "        \n",
    "    timestamp = datetime.datetime.today()\n",
    "     # Get rid of the %23 to create a valid filename\n",
    "    filename = hashtag[3:] + timestamp.strftime('%Y%m%d')\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        pickle.dump(tweets, f)\n",
    "        \n",
    "    return tweets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So this `hashtag_searcher()` function creates two lists, tweets and temp. When the api.search method is called, the results are returned to temp up to the api_rate_limit. Each of the results are then appended to the main, tweets, list, and the once the 15 minute interval has clicked away, the search renews. However, instead of starting over again, it starts at the max_id of the last returned value - it goes back to where it left off, in other words.\n",
    "\n",
    "Once the full count has run, the tweets list is both is pickled and returned - a belt and braces undertaking, in keeping with the long time these searches can take.\n",
    "```\n",
    "if __name__ == '__main__':\n",
    "    api = create_the_api(\"my_twitter_login.json\")\n",
    "    tweets = hashtag_searcher('ladyball', 1000, api)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I've pickled these `#ladyball` tweets since January 14th. So, what I'm going to do next is\n",
    "1. Load up the pickles from where they're stored into a single list, `tweets_prime`.\n",
    "2. Prune that list to remove the duplicates. Every tweet has its own unique id, identified as `id` or `id_str`. We'll use that to catch the spares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ladyballs = glob.glob(\"/Users/anthonymunnelly/Documents/TECH/Python/Tutorials/ladyball/ladyball---the-story-of-a-hashtag/pickled_ladyballs/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tweets_prime = []\n",
    "for ball in ladyballs:\n",
    "    with open(ball) as f:\n",
    "        temp = pickle.load(f)\n",
    "        [tweets_prime.append(t) for t in temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tweepy.models.Status'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9686"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print type(tweets_prime[0])\n",
    "len(tweets_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3774\n"
     ]
    }
   ],
   "source": [
    "id_watcher = []\n",
    "ladyballs = []\n",
    "for t in tweets_prime:\n",
    "    if t.id_str not in id_watcher:\n",
    "        ladyballs.append(t)\n",
    "        id_watcher.append(t.id_str)\n",
    "        \n",
    "print len(ladyballs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Looking at the Tweet Dates\n",
    "The easiest way to look at the tweet dates is to create a data frame and use that to pull from the `ladyballs` list such data as a data frame is unsuited to storing, such as text.\n",
    "My method of choice for creating dataframes is to create a dictionary and then turn that dictionary into a dataframe. And for creating dictionary, there's nothing as handy as the defaultdict in the collections module. It's so neat and tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "my_initial_dict = defaultdict(list)\n",
    "\n",
    "my_initial_dict['created_at'] = [l.created_at for l in ladyballs]\n",
    "my_initial_dict['screen_name'] = [l.user.screen_name for l in ladyballs]\n",
    "my_initial_dict['id_str'] = [l.id_str for l in ladyballs]\n",
    "my_initial_dict['followers_count'] = [l.user.followers_count for l in ladyballs]\n",
    "my_initial_dict['text'] = [l.text for l in ladyballs]\n",
    "my_initial_dict['description'] = [l.user.description for l in ladyballs]\n",
    "my_initial_dict['retweet_count'] = [l.retweet_count for l in ladyballs]\n",
    "\n",
    "df_ladyballs = pd.DataFrame(my_initial_dict, columns = ['created_at',\n",
    "                                                        'screen_name',\n",
    "                                                        'id_str',\n",
    "                                                        'followers_count',\n",
    "                                                        'text'\n",
    "                                                       'description',\n",
    "                                                       'retweet_count'])\n",
    "\n",
    "# Life is much handier if we sort the list by date.\n",
    "df_ladyballs.sort_values('created_at', inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Looking at the Life Cycle\n",
    "You can look at the first twenty or a hundred tweets on `Github` but take it from me, they were fairly negative. So let's look at the life cycle of the `ladyball` hashtag to see when people were talking about it, and we can do this by grouping by date - kind of. To group by date, we have to\n",
    "1. Create a new column on the dataframe, with a string representation of the date\n",
    "2. Group by this column.\n",
    "This is how that works out in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09 Jan 1\n",
      "11 Jan 1\n",
      "13 Jan 901\n",
      "14 Jan 966\n",
      "15 Jan 1208\n",
      "16 Jan 381\n",
      "17 Jan 75\n",
      "18 Jan 68\n",
      "19 Jan 90\n",
      "20 Jan 26\n",
      "21 Jan 36\n",
      "22 Jan 18\n",
      "23 Jan 3\n"
     ]
    }
   ],
   "source": [
    "day_month = [tweet.strftime('%d %b') for tweet in df_ladyballs['created_at']]\n",
    "df_ladyballs['day'] = day_month\n",
    "ladyballs_by_day = df_ladyballs.groupby('day')\n",
    "for a, b in ladyballs_by_day:\n",
    "    print a, b.created_at.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Looking at the Exposure\n",
    "It's not true to say that a tweet is only seen by someone's followers. A tweet can be quoted or retweeted, it can found in a search, it can be found when a hashtag is trending. All these things. But as there's no point in throwing up our hands and saying ah, who knows?, the convention has become to look at a tweet's reach, or exposure, as being dependent on that particular tweeter's followers.\n",
    "So, what is the follower distribution like among the `#ladyball` tweeters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x12b6792d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.boxplot(df_ladyballs.followers_count)\n",
    "plt.title('Followers Count of Ladyball Tweeters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![followers_count](boxplot1.png)\n",
    "That's one crazy-looking boxplot. Let's see the actual figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count       3774.000000\n",
      "mean        5543.403286\n",
      "std        95562.944061\n",
      "min            0.000000\n",
      "25%          236.000000\n",
      "50%          578.500000\n",
      "75%         1486.250000\n",
      "max      5738471.000000\n",
      "Name: followers_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print df_ladyballs.followers_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Isn't that interesting? 75% of our tweets, 2,530 of them, have follower counts of 1,486 or lower. The remaining 994 have over five and a half-million between them. Let's see who they are, by slicing the data frame into a `high_end`, those accounts with more than 1,486 followers, and `low_end`, the remaining majority of accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "high_end = df_ladyballs[df_ladyballs['followers_count'] > 1486]\n",
    "low_end = df_ladyballs[df_ladyballs['followers_count'] < 1486]\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].boxplot(low_end.followers_count.values)\n",
    "ax[0].set_title('Low End')\n",
    "ax[1].boxplot(high_end.followers_count.values)\n",
    "ax[1].set_title('High End')\n",
    "fig.subplots_adjust(wspace = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](boxplot2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Plotting Aside\n",
    "We shuld note that we're plotting `high_end.followers_count.values` rather than `high_end.followers_count`.\n",
    "This is because we've taken a slice of the original `df_ladyballs` data frame, this throws off the indexing,\n",
    "and the thrown indexing goes on to throw `matplotlib`. There may be a more elegant way around this problem than calling `values`, but calling `values` works and that's enough for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dealing with the Outliers\n",
    "There are still an enormous amount of outliers in the high end. Obviously, the 5.5 million is the biggest beast of all. Let's drop that and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "high_end_minus_outlier = high_end[high_end['followers_count'] < 5738471]\n",
    "\n",
    "%matplotlib qt\n",
    "fig, ax = plt.subplots(1,3)\n",
    "ax[0].boxplot(low_end.followers_count.values)\n",
    "ax[0].set_title('Low End')\n",
    "ax[1].boxplot(high_end.followers_count.values)\n",
    "ax[1].set_title('High End')\n",
    "ax[2].boxplot(high_end_minus_outlier.followers_count.values)\n",
    "ax[2].set_title('High End\\nMinus Outlier')\n",
    "fig.subplots_adjust(wspace = 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](boxplot3.png)\n",
    "And again we're looking at distribution that skews very strongly to the right. We still can't even make out the box in the boxplot, such the outlier dominance in the dataset.\n",
    "Let's look at the top tweets by `follower_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>id_str</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>textdescription</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3579</th>\n",
       "      <td>2016-01-19 21:39:25</td>\n",
       "      <td>washingtonpost</td>\n",
       "      <td>689562834330611712</td>\n",
       "      <td>5738471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>19 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>2016-01-15 18:00:29</td>\n",
       "      <td>Vibra1049</td>\n",
       "      <td>688058186528522240</td>\n",
       "      <td>545959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>15 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>2016-01-16 22:29:05</td>\n",
       "      <td>manuel_c</td>\n",
       "      <td>688488168140713984</td>\n",
       "      <td>422373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>16 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>2016-01-13 21:48:22</td>\n",
       "      <td>SimonHoneydew</td>\n",
       "      <td>687390760174489600</td>\n",
       "      <td>293945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>13 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <td>2016-01-15 17:39:29</td>\n",
       "      <td>spin1038</td>\n",
       "      <td>688052899390341120</td>\n",
       "      <td>252379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>15 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2847</th>\n",
       "      <td>2016-01-15 18:01:09</td>\n",
       "      <td>spin1038</td>\n",
       "      <td>688058353411616771</td>\n",
       "      <td>252379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>15 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>2016-01-15 20:13:03</td>\n",
       "      <td>spin1038</td>\n",
       "      <td>688091547414335490</td>\n",
       "      <td>252379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>15 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>2016-01-13 18:41:23</td>\n",
       "      <td>Independent_ie</td>\n",
       "      <td>687343701123792897</td>\n",
       "      <td>250647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>13 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>2016-01-14 10:48:21</td>\n",
       "      <td>Independent_ie</td>\n",
       "      <td>687587048212029440</td>\n",
       "      <td>250646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>14 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>2016-01-13 23:16:42</td>\n",
       "      <td>Independent_ie</td>\n",
       "      <td>687412987989213184</td>\n",
       "      <td>250646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>13 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>2016-01-13 16:53:20</td>\n",
       "      <td>JOEdotie</td>\n",
       "      <td>687316511925616640</td>\n",
       "      <td>231216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>13 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>2016-01-19 12:48:24</td>\n",
       "      <td>PaulOMahonyEire</td>\n",
       "      <td>689429199220662273</td>\n",
       "      <td>183369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>19 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3046</th>\n",
       "      <td>2016-01-15 17:27:00</td>\n",
       "      <td>RTEgaa</td>\n",
       "      <td>688049757823053825</td>\n",
       "      <td>156799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>15 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>2016-01-15 17:49:19</td>\n",
       "      <td>RTEgaa</td>\n",
       "      <td>688055377003417600</td>\n",
       "      <td>156799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>15 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>2016-01-14 22:15:26</td>\n",
       "      <td>RossOCK</td>\n",
       "      <td>687759956095971329</td>\n",
       "      <td>155383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>14 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696</th>\n",
       "      <td>2016-01-18 09:15:34</td>\n",
       "      <td>TheDrum</td>\n",
       "      <td>689013250718044160</td>\n",
       "      <td>150191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>18 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>2016-01-17 17:15:11</td>\n",
       "      <td>TheDrum</td>\n",
       "      <td>688771560799338496</td>\n",
       "      <td>150191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>17 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>2016-01-20 22:45:34</td>\n",
       "      <td>TheDrum</td>\n",
       "      <td>689941866108850177</td>\n",
       "      <td>150191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>20 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>2016-01-16 12:20:22</td>\n",
       "      <td>TheDrum</td>\n",
       "      <td>688334979928080384</td>\n",
       "      <td>149758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>16 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>2016-01-15 22:35:18</td>\n",
       "      <td>TheDrum</td>\n",
       "      <td>688127345329328128</td>\n",
       "      <td>149758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>15 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3109</th>\n",
       "      <td>2016-01-15 14:52:50</td>\n",
       "      <td>TheDrum</td>\n",
       "      <td>688010961375096834</td>\n",
       "      <td>149758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>15 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>2016-01-15 18:49:46</td>\n",
       "      <td>TheDrum</td>\n",
       "      <td>688070587886006272</td>\n",
       "      <td>149758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>15 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>2016-01-16 01:02:01</td>\n",
       "      <td>NewstalkFM</td>\n",
       "      <td>688164266944274433</td>\n",
       "      <td>127998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>16 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>2016-01-15 17:58:19</td>\n",
       "      <td>NewstalkFM</td>\n",
       "      <td>688057640019202048</td>\n",
       "      <td>127998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>15 Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>2016-01-15 19:37:22</td>\n",
       "      <td>NewstalkFM</td>\n",
       "      <td>688082566948220929</td>\n",
       "      <td>127998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>15 Jan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              created_at      screen_name              id_str  \\\n",
       "3579 2016-01-19 21:39:25   washingtonpost  689562834330611712   \n",
       "2855 2016-01-15 18:00:29        Vibra1049  688058186528522240   \n",
       "1762 2016-01-16 22:29:05         manuel_c  688488168140713984   \n",
       "1171 2016-01-13 21:48:22    SimonHoneydew  687390760174489600   \n",
       "3003 2016-01-15 17:39:29         spin1038  688052899390341120   \n",
       "2847 2016-01-15 18:01:09         spin1038  688058353411616771   \n",
       "2388 2016-01-15 20:13:03         spin1038  688091547414335490   \n",
       "1471 2016-01-13 18:41:23   Independent_ie  687343701123792897   \n",
       "543  2016-01-14 10:48:21   Independent_ie  687587048212029440   \n",
       "914  2016-01-13 23:16:42   Independent_ie  687412987989213184   \n",
       "1633 2016-01-13 16:53:20         JOEdotie  687316511925616640   \n",
       "3613 2016-01-19 12:48:24  PaulOMahonyEire  689429199220662273   \n",
       "3046 2016-01-15 17:27:00           RTEgaa  688049757823053825   \n",
       "2937 2016-01-15 17:49:19           RTEgaa  688055377003417600   \n",
       "3381 2016-01-14 22:15:26          RossOCK  687759956095971329   \n",
       "3696 2016-01-18 09:15:34          TheDrum  689013250718044160   \n",
       "3725 2016-01-17 17:15:11          TheDrum  688771560799338496   \n",
       "3525 2016-01-20 22:45:34          TheDrum  689941866108850177   \n",
       "1924 2016-01-16 12:20:22          TheDrum  688334979928080384   \n",
       "2221 2016-01-15 22:35:18          TheDrum  688127345329328128   \n",
       "3109 2016-01-15 14:52:50          TheDrum  688010961375096834   \n",
       "2589 2016-01-15 18:49:46          TheDrum  688070587886006272   \n",
       "2100 2016-01-16 01:02:01       NewstalkFM  688164266944274433   \n",
       "2872 2016-01-15 17:58:19       NewstalkFM  688057640019202048   \n",
       "2456 2016-01-15 19:37:22       NewstalkFM  688082566948220929   \n",
       "\n",
       "      followers_count textdescription  retweet_count     day  \n",
       "3579          5738471             NaN             30  19 Jan  \n",
       "2855           545959             NaN              1  15 Jan  \n",
       "1762           422373             NaN              0  16 Jan  \n",
       "1171           293945             NaN             21  13 Jan  \n",
       "3003           252379             NaN              5  15 Jan  \n",
       "2847           252379             NaN              1  15 Jan  \n",
       "2388           252379             NaN              2  15 Jan  \n",
       "1471           250647             NaN              4  13 Jan  \n",
       "543            250646             NaN              2  14 Jan  \n",
       "914            250646             NaN              2  13 Jan  \n",
       "1633           231216             NaN              3  13 Jan  \n",
       "3613           183369             NaN              0  19 Jan  \n",
       "3046           156799             NaN             10  15 Jan  \n",
       "2937           156799             NaN             11  15 Jan  \n",
       "3381           155383             NaN              6  14 Jan  \n",
       "3696           150191             NaN              1  18 Jan  \n",
       "3725           150191             NaN              2  17 Jan  \n",
       "3525           150191             NaN              4  20 Jan  \n",
       "1924           149758             NaN              1  16 Jan  \n",
       "2221           149758             NaN              4  15 Jan  \n",
       "3109           149758             NaN              5  15 Jan  \n",
       "2589           149758             NaN              4  15 Jan  \n",
       "2100           127998             NaN              0  16 Jan  \n",
       "2872           127998             NaN              3  15 Jan  \n",
       "2456           127998             NaN              3  15 Jan  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_end_examined = high_end.sort_values('followers_count', ascending = False)\n",
    "high_end_examined.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And there are some huge brands in there. Newtalk, the Indo, TheDrum, and biggest of all, The Washington Post, with five and a half-million followers. We need to go back to the original data to see the text descriptions of the accounts and the tweets in the top ten tweets here. We can use `id_str` to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent_ie\n",
      "Followers: 250,646\n",
      "Official Twitter for Independent.ie, providing #breaking news content to 10million readers a month.\n",
      "Get in touch: contact@independent.ie, #IndoSubmit\n",
      "2016-01-14 10:48:21\n",
      "RT @IndoSport: The 'Pink Ladyball' - innovative marketing at its finest: https://t.co/k4D2ED40Eo #LadyBall https://t.co/v2tZ60sDdK\n",
      "\n",
      "\n",
      "Independent_ie\n",
      "Followers: 250,646\n",
      "2016-01-13 23:16:42\n",
      "#Ladyball sparks strong reaction on Twitter.... https://t.co/Bow2tLfdcw\n",
      "\n",
      "\n",
      "SimonHoneydew\n",
      "Followers: 293,945\n",
      "YouTuber, Cosplayer, Dancer, World #1 Anime Fan, Make-up Artist Tutor, League of Legends Pro Streamer, Podcaster, Co-Founder of Yogscast\n",
      "2016-01-13 21:48:22\n",
      "RT @theladyball_com: Don't break a nail, break boundaries with #Ladyball https://t.co/Ixpmn4Bky1\n",
      "\n",
      "\n",
      "Independent_ie\n",
      "Followers: 250,647\n",
      "2016-01-13 18:41:23\n",
      "RT @IndoSport: Pink 'Ladyball' for women sparks strong reaction on Twitter https://t.co/mBc1Gdn0Bb #Ladyball https://t.co/FrJyFi1x6d\n",
      "\n",
      "\n",
      "manuel_c\n",
      "Followers: 422,373\n",
      "Luxury product consultant, social media strategist/addict. All about brand/customer experience. Food,travel & bourbon buff, red tape nemesis. Personal account.\n",
      "2016-01-16 22:29:05\n",
      "This Genius Marketing Campaign Says Everything About Women's Sports https://t.co/6itHoFsx3K #ladyball\n",
      "\n",
      "\n",
      "spin1038\n",
      "Followers: 252,379\n",
      "No.1 for Music & Entertainment\n",
      "2016-01-15 20:13:03\n",
      "The real meaning behind the #Ladyball - https://t.co/MT43vnMAtB\n",
      "\n",
      "\n",
      "spin1038\n",
      "Followers: 252,379\n",
      "2016-01-15 18:01:09\n",
      "It was Lidl trolling us with those #Ladyball ads! https://t.co/yX7ltArFGB\n",
      "\n",
      "\n",
      "Vibra1049\n",
      "Followers: 545,959\n",
      "La mejor música en español: pop, baladas, música para planchar y romántica.\n",
      "2016-01-15 18:00:29\n",
      "¿Te sentirías más atraída por los deportes si los balones fueran rosados  #Ladyball? Aquí https://t.co/daGzpKmYVh https://t.co/qyEbtGbbpp\n",
      "\n",
      "\n",
      "spin1038\n",
      "Followers: 252,379\n",
      "2016-01-15 17:39:29\n",
      "The message behind the #Ladyball revealed - https://t.co/XAU59zFiXW https://t.co/oFdN7Jt7GW\n",
      "\n",
      "\n",
      "washingtonpost\n",
      "Followers: 5,738,471\n",
      "Tweet-length breaking news, analysis from around the world. Founded in 1877. Follow our journalists on Twitter: https://t.co/VV0UBAMHg8\n",
      "2016-01-19 21:39:25\n",
      "What does the fake #ladyball campaign say about women’s sports? https://t.co/t4TLm97Lr6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_ids = list(high_end_examined.id_str)[:10]\n",
    "description_catcher = [] #So we don't print the descriptions more than once.\n",
    "for tweet in ladyballs:\n",
    "    if tweet.id_str in my_ids:\n",
    "        print tweet.user.screen_name\n",
    "        print \"Followers: {:,}\".format(tweet.user.followers_count)\n",
    "        if tweet.user.description not in description_catcher:\n",
    "            print tweet.user.description\n",
    "            description_catcher.append(tweet.user.description)\n",
    "        print tweet.created_at\n",
    "        print tweet.text\n",
    "        print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Chatter v Reach\n",
    "Now we're coming to the heart of Twitter. The fundamental question you have to answer, as you're trying to get your message out on Twitter, is this: Does it matter who's talking about you? Are many tweets by tweeters with small amounts of followers as good as a single tweet by a user with a huge amount of followers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Time v Reach\n",
    "This is our final graph of the evening. We're plotting the reach of the `#ladyball` hashtag against the number of times it was mentioned. We'll plot on different axes as it makes for a better visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.text.Text at 0x1270061d0>,\n",
       " <matplotlib.text.Text at 0x12903e810>,\n",
       " <matplotlib.text.Text at 0x135f0a6d0>,\n",
       " <matplotlib.text.Text at 0x135f0ae10>,\n",
       " <matplotlib.text.Text at 0x135f0e590>,\n",
       " <matplotlib.text.Text at 0x135f0ecd0>,\n",
       " <matplotlib.text.Text at 0x135f11450>,\n",
       " <matplotlib.text.Text at 0x135f11b90>,\n",
       " <matplotlib.text.Text at 0x135edf1d0>,\n",
       " <matplotlib.text.Text at 0x135eda310>,\n",
       " <matplotlib.text.Text at 0x12b600350>,\n",
       " <matplotlib.text.Text at 0x1295e6750>,\n",
       " <matplotlib.text.Text at 0x129d3ff90>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_count = []\n",
    "tweet_reach = []\n",
    "for a, b in ladyballs_by_day:\n",
    "    tweet_count.append(b.created_at.count())\n",
    "    tweet_reach.append(b.followers_count.sum())\n",
    "\n",
    "days = []\n",
    "[days.append(a) for a, b in ladyballs_by_day]\n",
    "%matplotlib qt\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(tweet_count, \"r-\", label = 'count')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(tweet_reach, 'b--', label = 'reach')\n",
    "ax1.set_xticks(range(len(days)))\n",
    "ax1.set_xticklabels(days, rotation = 45)\n",
    "# http://matplotlib.org/examples/api/two_scales.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](figure_1.png)\n",
    "So here we can see that reach is reasonably proportional to chatter / number of tweets during the peak of the `#ladyball` hashtag's life cycle, but the hashtag reaches its highest reach just as it's dying off and people aren't interested anymore. These are the numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\tCount\tReach\n",
      "09 Jan\t    1\t        48\n",
      "11 Jan\t    1\t     5,312\n",
      "13 Jan\t  901\t 3,092,344\n",
      "14 Jan\t  966\t 3,122,821\n",
      "15 Jan\t 1208\t 5,644,684\n",
      "16 Jan\t  381\t 1,554,422\n",
      "17 Jan\t   75\t   338,193\n",
      "18 Jan\t   68\t   293,447\n",
      "19 Jan\t   90\t 6,451,310\n",
      "20 Jan\t   26\t   186,680\n",
      "21 Jan\t   36\t   189,709\n",
      "22 Jan\t   18\t    31,835\n",
      "23 Jan\t    3\t     9,999\n"
     ]
    }
   ],
   "source": [
    "counter = len(tweet_count)\n",
    "print \"Date\\tCount\\tReach\"\n",
    "for c in range(counter):\n",
    "    print \"{}\\t{:>5}\\t{:>10,}\".format(days[c], tweet_count[c], tweet_reach[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So. Was it Worth It?\n",
    "Was the `#ladyball` tweet worth the controversy it caused?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Yes\n",
    "It got people talking, which is the point of any publicity and/or advertising campaign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Yes\n",
    "The initial negative feedback was hugely overweighed by the positive mentions, especially the one in the [Washington Post](http://www.washingtonpost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### No\n",
    "This is a little meta but: `#ladyball` was a success as a teaser to the campaign. But once the true point of `#ladyball` was revealed, what was left for people to be upset about, or happy about, or engaged with? There was no way to build from `#ladyball` on to the avowed purpose of it all, Lidl's sponsorship of ladies' football. The Washington Post piece was very worthy and is sure to be front-and-centre when the advertising agency put their bill together, but in what way did the `#ladyball`campaign get more people interested in ladies' Gaelic football or get more people shopping at Lidl? That case isn't proven."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
